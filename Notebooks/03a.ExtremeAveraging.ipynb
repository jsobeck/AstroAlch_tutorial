{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3a: Extreme Averaging\n",
    "\n",
    "In this notebook we will set up a huge single column database and take the average of the numbers in it. The goal is to stress test `pandas` ability to work with large datasets.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "You will need to have installed `sqlalchemy` and `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import sqlalchemy as sq\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.sql import func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dbfile = 'huge.db'\n",
    "\n",
    "try: os.remove(dbfile)\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Base = declarative_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Number(Base):\n",
    "    __tablename__ = 'number'\n",
    "    \n",
    "    id = sq.Column(sq.Integer, primary_key=True)\n",
    "    value = sq.Column(sq.Float, nullable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "engine = sq.create_engine('sqlite:///'+dbfile)\n",
    "Base.metadata.create_all(engine)\n",
    "\n",
    "Base.metadata.bind = engine\n",
    "DBSession = sessionmaker(bind=engine)\n",
    "session = DBSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to insert data into our database. We'll use a low level insertion command to fill the table fast - you can read more about how this works [here](http://docs.sqlalchemy.org/en/latest/faq/performance.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = int(1e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.92 ms, sys: 1.57 ms, total: 11.5 ms\n",
      "Wall time: 15.2 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x1067ac3d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "engine.execute(\n",
    "    Number.__table__.insert(),\n",
    "        [{\"value\": x} for x in xrange(N)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, below is some code that is several hundred times slower:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.57 s, sys: 726 ms, total: 2.29 s\n",
      "Wall time: 4.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "j = N\n",
    "while j < 2*N:\n",
    "    session.add(Number(value=float(j)))\n",
    "    session.commit()\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At about 20ms per 1000 numbers, we should be able to make a 10 billion row database in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.6 hours\n"
     ]
    }
   ],
   "source": [
    "N = int(1e10)\n",
    "print np.round((N * ((20*1e-3) / 1e3) / 3600.0), 1),\"hours\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try for something smaller: we should be able to do 10 million rows in a few minutes. We already inserted 2000 rows above, so there's no need to do them again! The resulting db should be about 100Mb in size. Then let's see how `pandas` gets on with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = int(1e7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.5 s, sys: 3.94 s, total: 53.4 s\n",
      "Wall time: 54.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.result.ResultProxy at 0x2b778c650>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "engine.execute(\n",
    "    Number.__table__.insert(),\n",
    "        [{\"value\": x} for x in xrange(2000, N)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124M\thuge.db\r\n"
     ]
    }
   ],
   "source": [
    "!du -h $dbfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying Data and Estimating the Mean\n",
    "\n",
    "Let's do this two different ways, using the built-in SQL `avg` function, and `pandas` plus `numpy`.\n",
    "\n",
    "In this function we use the `time` package to measure wallclock time, and `guppy` to measure memory usage.\n",
    "\n",
    "We expect the mean to be equal to `0.5*(N-1)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4999999.5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.5*(N-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def estimate_mean(method='sql_function'):\n",
    "    \n",
    "    import time as wallclock\n",
    "    import guppy\n",
    "    \n",
    "    measure = guppy.hpy()\n",
    "    \n",
    "    start, end = {}, {}\n",
    "    start['memory'] = measure.heap().size\n",
    "    start['time'] = wallclock.time()\n",
    "    \n",
    "    df, mean = None, None\n",
    "\n",
    "    if method == 'pandas_query':\n",
    "        df = pd.read_sql(session.query(Number.value).statement, session.bind) \n",
    "        mean = np.mean(df.value)\n",
    "        \n",
    "    elif method == 'sql_function':\n",
    "        mean = session.query(func.avg(Number.value)).one()[0]\n",
    "        \n",
    "    \n",
    "    end['time'] = wallclock.time()\n",
    "    end['memory'] = measure.heap().size\n",
    "\n",
    "    time = (end['time']-start['time'])\n",
    "    memory = (end['memory']-start['memory']) / (1024.0*1024.0)\n",
    "\n",
    "    print \"Estimated mean distance = \", mean\n",
    "\n",
    "    print \"Wallclock time spent = \", np.round(time,1), \"seconds\"\n",
    "    print \"Memory used = \",np.round(memory,1), \"Mb\"\n",
    "    \n",
    "    del df, mean\n",
    "\n",
    "    return time, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated mean distance =  4999999.5\n",
      "Wallclock time spent =  34.0 seconds\n",
      "Memory used =  0.0 Mb\n"
     ]
    }
   ],
   "source": [
    "t, m = estimate_mean(method='pandas_query')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated mean distance =  4999999.5\n",
      "Wallclock time spent =  1.9 seconds\n",
      "Memory used =  0.1 Mb\n"
     ]
    }
   ],
   "source": [
    "t, m = estimate_mean(method='sql_function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "It's possible to work with 10 million row databases using `SQLalchemy` and `pandas` - I was unable to go 10 times bigger without the kernel dying. Taking the average of 10 million numbers seems to be an order of magnitude faster with the built-in command than with `np.mean()` operating on a `pandas` dataframe. Neither approach seems to need much memory at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try: os.remove(dbfile)\n",
    "except: pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
